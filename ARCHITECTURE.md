# ğŸ—ï¸ Netflix ETL Pipeline - Architecture Documentation

## Tá»•ng Quan Kiáº¿n TrÃºc

Netflix ETL Pipeline Ä‘Æ°á»£c thiáº¿t káº¿ theo mÃ´ hÃ¬nh **3-layer architecture**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DATA PRESENTATION LAYER                 â”‚
â”‚  (Jupyter Notebooks, SQL Queries, Analytics Tools)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DATABASE LAYER                          â”‚
â”‚  PostgreSQL (dim_movies, dim_genres, movies_genres) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ETL PROCESSING LAYER                    â”‚
â”‚  (Extract â†’ Transform â†’ Load Pipeline)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DATA SOURCE LAYER                       â”‚
â”‚  (CSV File / Kaggle Dataset)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. Data Source Layer

### Chá»©c nÄƒng

Quáº£n lÃ½ dá»¯ liá»‡u thÃ´ tá»« cÃ¡c nguá»“n:

```
Input Sources:
â”œâ”€â”€ ğŸ“„ netflix_titles.csv (Kaggle Dataset)
â”‚   â”œâ”€â”€ 5,500+ rows
â”‚   â”œâ”€â”€ 12 columns
â”‚   â””â”€â”€ ~350 KB size
â””â”€â”€ ğŸŒ Kaggle API (optional download)
```

### Äá»‹nh dáº¡ng Dá»¯ liá»‡u ThÃ´

| Column       | Type   | Description                  |
| ------------ | ------ | ---------------------------- |
| show_id      | string | Unique identifier            |
| type         | string | "Movie" or "TV Show"         |
| title        | string | Title                        |
| director     | string | Director(s) - may have NA    |
| cast         | string | Cast list                    |
| country      | string | Country (may have NA)        |
| date_added   | string | Date added - may have NA     |
| release_year | int    | Year                         |
| rating       | string | Content rating - may have NA |
| duration     | string | Duration                     |
| listed_in    | string | **Genres (comma-separated)** |
| description  | string | Description                  |

### Module

**File:** `src/extractor.py`

**Lá»›p:** `NetflixExtractor`

- `extract_from_csv()` - Äá»c CSV
- `extract_from_kaggle()` - Download tá»« Kaggle API
- `validate_data()` - Kiá»ƒm tra validity
- `get_data_info()` - In thÃ´ng tin

---

## 2. ETL Processing Layer

### 2.1 Extract Phase

```
CSV File
    â†“
[Read with Pandas]
    â†“
Raw DataFrame
    â†“
[Validate Schema]
    â†“
âœ“ Ready for Transform
```

**Input:** `netflix_titles.csv` (5,500 rows Ã— 12 columns)
**Output:** Pandas DataFrame (raw data)
**Validation:** Check required columns exist

---

### 2.2 Transform Phase

Gá»“m 4 bÆ°á»›c chÃ­nh:

#### 2.2.1 Data Cleaning

```
Raw Data (5,500 rows)
    â†“
[Drop NA in: director, country, date_added, rating]
    â†“
[Drop duplicates]
    â†“
Cleaned Data (~4,800 rows)
```

**XÃ³a:**

- Rows vá»›i NA trong `director`
- Rows vá»›i NA trong `country`
- Rows vá»›i NA trong `date_added`
- Rows vá»›i NA trong `rating`
- Duplicate rows

**Káº¿t quáº£:** Giáº£m 10-12% rows, tÄƒng data quality

---

#### 2.2.2 Text Normalization

```
"  director1, director2  "
    â†“
[Strip whitespace]
    â†“
"director1, director2"
```

**Applied to:**

- director
- country
- listed_in (genres)
- title

---

#### 2.2.3 Date Normalization

```
"January 1, 2021"
    â†“
[Parse to datetime]
    â†“
2021-01-01
    â†“
[Format as YYYY-MM-DD]
    â†“
"2021-01-01"
```

**Transform:** date_added column â†’ Standard format

---

#### 2.2.4 Genre Explosion (Explode)

**TrÆ°á»›c:**

```
show_id: 1, title: "Movie A", listed_in: "Action, Comedy, Drama"
```

**Sau (.explode()):**

```
show_id: 1, title: "Movie A", listed_in: "Action"
show_id: 1, title: "Movie A", listed_in: "Comedy"
show_id: 1, title: "Movie A", listed_in: "Drama"
```

**TÃ¡c dá»¥ng:**

- Táº¡o má»‘i quan há»‡ 1:N
- Chuáº©n bá»‹ cho Star Schema
- Má»—i row = 1 movie-genre pair

**Module:** `src/transformer.py`

**Lá»›p:** `NetflixTransformer`

```python
def clean_data()           # XÃ³a NA & duplicates
def normalize_dates()      # Chuáº©n hÃ³a ngÃ y
def normalize_text()       # Strip whitespace
def explode_genres()       # TÃ¡ch genres
def create_star_schema()   # Táº¡o 3 báº£ng
def transform()            # Cháº¡y táº¥t cáº£
```

---

### 2.3 Star Schema Creation

Chuyá»ƒn tá»« flat structure â†’ normalized relational model

#### 2.3.1 Dimension Table: dim_movies

```sql
CREATE TABLE dim_movies (
    movie_id SERIAL PRIMARY KEY,
    title VARCHAR(255),
    type VARCHAR(50),
    director VARCHAR(500),
    country VARCHAR(500),
    date_added DATE,
    release_year INTEGER,
    rating VARCHAR(20),
    duration VARCHAR(50),
    description TEXT,
    created_at TIMESTAMP
);
```

**Äáº·c Ä‘iá»ƒm:**

- Primary key: `movie_id` (auto-increment)
- 1 row per unique show_id
- KhÃ´ng cÃ³ duplicate titles (unique show_id)
- Chá»©a táº¥t cáº£ thÃ´ng tin phim

**Rows:** ~4,800 (duy nháº¥t)

---

#### 2.3.2 Dimension Table: dim_genres

```sql
CREATE TABLE dim_genres (
    genre_id SERIAL PRIMARY KEY,
    genre_name VARCHAR(100) UNIQUE
);
```

**Äáº·c Ä‘iá»ƒm:**

- Primary key: `genre_id`
- Constraint: `UNIQUE` trÃªn `genre_name`
- Loáº¡i bá» duplicate genres
- Danh sÃ¡ch tham chiáº¿u

**Rows:** ~25 (duy nháº¥t genres)

**Example:**

```
1 | Action & Adventure
2 | Anime
3 | Children & Family
...
25 | Stand-Up Comedy
```

---

#### 2.3.3 Junction Table: movies_genres

```sql
CREATE TABLE movies_genres (
    movie_id INTEGER (FK -> dim_movies),
    genre_id INTEGER (FK -> dim_genres),
    PRIMARY KEY (movie_id, genre_id)
);
```

**Äáº·c Ä‘iá»ƒm:**

- Composite Primary Key: (movie_id, genre_id)
- Foreign Key: movie_id â†’ dim_movies
- Foreign Key: genre_id â†’ dim_genres
- Má»‘i quan há»‡ N:N

**Rows:** ~10,000+ (má»‘i quan há»‡)

**Example:**

```
movie_id | genre_id
---------|----------
1        | 1  (Movie 1 is Action)
1        | 5  (Movie 1 is Comedy)
2        | 1  (Movie 2 is Action)
...
```

---

### 2.4 Schema Diagram

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   dim_genres     â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚ genre_id (PK)    â”‚
                    â”‚ genre_name (U)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚ FK
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ movies_genres    â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚ movie_id (FK)    â”‚â—„â”€â”€â”€â”€â”
                    â”‚ genre_id (FK)    â”‚     â”‚
                    â”‚ PK: (m_id,g_id)  â”‚     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ FK
                                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
                    â”‚   dim_movies     â”‚     â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”‚
                    â”‚ movie_id (PK)â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”˜
                    â”‚ title            â”‚
                    â”‚ type             â”‚
                    â”‚ director         â”‚
                    â”‚ country          â”‚
                    â”‚ date_added       â”‚
                    â”‚ release_year     â”‚
                    â”‚ rating           â”‚
                    â”‚ duration         â”‚
                    â”‚ description      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2.5 Load Phase

```
Transformed Data
  (3 DataFrames)
    â†“
[Create Connection]
    â†“
[Load dim_genres] â† pháº£i load trÆ°á»›c (FK dependency)
    â†“
[Load dim_movies]
    â†“
[Load movies_genres]
    â†“
[Validate Load]
    â†“
âœ“ Data in Database
```

**Module:** `src/loader.py`

**Lá»›p:** `NetflixLoader`

```python
def connect()              # Káº¿t ná»‘i PostgreSQL
def load_dim_genres()      # Táº£i genres
def load_dim_movies()      # Táº£i movies
def load_movies_genres()   # Táº£i relationships
def load_all()             # Táº£i táº¥t cáº£ 3 báº£ng
def validate_load()        # Kiá»ƒm tra dá»¯ liá»‡u
def disconnect()           # ÄÃ³ng káº¿t ná»‘i
```

**Methods Ä‘á»ƒ tá»‘i Æ°u:**

- `to_sql()` vá»›i Pandas - Chuáº©n vÃ  dá»… dÃ¹ng
- `COPY` command - Nhanh hÆ¡n (bulk insert)
- Truncate cÅ© â†’ Append má»›i (idempotent)

---

## 3. Database Layer

### 3.1 PostgreSQL Container

```yaml
# docker-compose.yml
services:
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: netflix_db
      POSTGRES_USER: netflix_user
      POSTGRES_PASSWORD: netflix_password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./docker/init.sql:/docker-entrypoint-initdb.d/init.sql
```

**TÃ­nh nÄƒng:**

- Alpine image (nhá», nhanh)
- Auto-init schema tá»« `init.sql`
- Persistent volume
- Health check

---

### 3.2 Indexing Strategy

```sql
-- Improve query performance
CREATE INDEX idx_movies_type ON dim_movies(type);
CREATE INDEX idx_movies_release_year ON dim_movies(release_year);
CREATE INDEX idx_movies_rating ON dim_movies(rating);
CREATE INDEX idx_genres_name ON dim_genres(genre_name);
```

**TÃ¡c dá»¥ng:**

- TÄƒng tá»‘c Ä‘á»™ WHERE queries
- Giáº£m full table scan
- Trade-off: Insert/update cháº­m hÆ¡n

---

### 3.3 Connection Management

```python
# SQLAlchemy Connection String
DATABASE_URL = f"postgresql://{user}:{password}@{host}:{port}/{database}"

# Create engine
engine = create_engine(DATABASE_URL)

# Use connection
with engine.connect() as connection:
    result = connection.execute(text("SELECT * FROM dim_movies"))
```

**Lá»£i Ã­ch:**

- Connection pooling
- Automatic cleanup
- Error handling

---

## 4. Data Presentation Layer

### 4.1 Jupyter Notebook Interface

```
notebooks/netflix_etl_pipeline.ipynb
â”‚
â”œâ”€â”€ ğŸ“Œ Setup & Imports
â”‚   â”œâ”€â”€ Import libraries
â”‚   â”œâ”€â”€ Set Python path
â”‚   â””â”€â”€ Load modules
â”‚
â”œâ”€â”€ ğŸ“¥ EXTRACT
â”‚   â”œâ”€â”€ Initialize Extractor
â”‚   â”œâ”€â”€ Read CSV
â”‚   â”œâ”€â”€ Display info
â”‚   â””â”€â”€ Validate data
â”‚
â”œâ”€â”€ ğŸ§¹ TRANSFORM
â”‚   â”œâ”€â”€ Initialize Transformer
â”‚   â”œâ”€â”€ Run transformation
â”‚   â”œâ”€â”€ Create Star Schema
â”‚   â””â”€â”€ Quality checks
â”‚
â”œâ”€â”€ ğŸ’¾ LOAD
â”‚   â”œâ”€â”€ Connect to DB
â”‚   â”œâ”€â”€ Load 3 tables
â”‚   â”œâ”€â”€ Validate load
â”‚   â””â”€â”€ Disconnect
â”‚
â””â”€â”€ ğŸ“Š ANALYSIS
    â”œâ”€â”€ Movies vs TV Shows
    â”œâ”€â”€ Top genres
    â”œâ”€â”€ Release year trends
    â”œâ”€â”€ Rating distribution
    â””â”€â”€ Country analysis
```

**Advantages:**

- Interactive development
- Step-by-step execution
- Built-in visualization
- Documentation + code combined

---

### 4.2 SQL Queries

```sql
-- Analytics layer
SELECT dg.genre_name, COUNT(*) as count
FROM dim_genres dg
JOIN movies_genres mg ON dg.genre_id = mg.genre_id
GROUP BY dg.genre_id, dg.genre_name
ORDER BY count DESC;
```

**Use cases:**

- Ad-hoc analysis
- Report generation
- Data exploration
- BI tool integration

---

## 5. Configuration Management

### 5.1 Environment Variables

```python
# config/config.py
class Config:
    DB_HOST = os.getenv("DB_HOST", "localhost")
    DB_PORT = os.getenv("DB_PORT", "5432")
    DB_NAME = os.getenv("DB_NAME", "netflix_db")
    DB_USER = os.getenv("DB_USER", "netflix_user")
    DB_PASSWORD = os.getenv("DB_PASSWORD", "netflix_password")

    DATA_PATH = os.getenv("DATA_PATH", "./data/netflix_titles.csv")

    DATABASE_URL = f"postgresql://..."
```

**Benefits:**

- Externalize configuration
- Environment-specific settings
- Secrets management
- Easy to modify without code change

---

## 6. Error Handling & Validation

### 6.1 Extraction Errors

```python
try:
    df = extractor.extract_from_csv()
except FileNotFoundError:
    # Handle missing file
    print("CSV file not found, try Kaggle API")
except Exception as e:
    print(f"Error reading CSV: {str(e)}")
```

---

### 6.2 Transformation Errors

```python
# Validate data after cleaning
if df[critical_cols].isnull().any():
    raise ValueError("Critical columns have NULL values")
```

---

### 6.3 Load Errors

```python
try:
    loader.connect()
except ConnectionError:
    print("Cannot connect to PostgreSQL")
    print("Ensure Docker container is running: docker-compose up -d")
```

---

## 7. Performance Optimization

### 7.1 Data Processing

| Operation         | Optimization                                   |
| ----------------- | ---------------------------------------------- |
| Read CSV          | Use `chunksize` untuk file besar               |
| String operations | Gunakan `.str.` methods (vectorized)           |
| Filtering         | Hindari `iterrows()`, gunakan boolean indexing |
| Groupby           | Optimize group size                            |

### 7.2 Database

| Optimization        | Impact                |
| ------------------- | --------------------- |
| Indexes             | ~10x faster SELECT    |
| Bulk insert         | ~5x faster INSERT     |
| Connection pooling  | Mengurangi overhead   |
| Prepared statements | Prevent SQL injection |

---

## 8. Scalability Considerations

### 8.1 Untuk Dataset Besar

1. **Chunked Processing**

   ```python
   # Read large CSV in chunks
   for chunk in pd.read_csv('file.csv', chunksize=10000):
       process_chunk(chunk)
   ```

2. **Distributed Processing**

   - Spark untuk parallel processing
   - Dask untuk out-of-core computation

3. **Incremental Loading**
   - Load only new data
   - Append vs replace

### 8.2 Untuk Many Users

1. **Connection Pooling**

   - Manage connection limits

2. **Caching**

   - Cache frequently accessed data
   - Redis untuk hot data

3. **Read Replicas**
   - Separate read/write databases

---

## 9. Monitoring & Maintenance

### 9.1 Health Checks

```bash
# Check database
docker-compose exec postgres psql -U netflix_user -d netflix_db -c "SELECT COUNT(*) FROM dim_movies;"

# Check application
python src/etl_pipeline.py --validate
```

### 9.2 Logging

```python
import logging

logger = logging.getLogger(__name__)
logger.info("Processing started")
logger.error("Error occurred", exc_info=True)
```

---

## 10. Security Best Practices

1. **Secrets Management**

   - Use `.env` file (not committed)
   - Use environment variables
   - Never hardcode passwords

2. **SQL Injection Prevention**

   - Use parameterized queries
   - SQLAlchemy prevents this automatically

3. **Access Control**
   - PostgreSQL user roles
   - Database-level permissions
   - Row-level security (if needed)

---

## Workflow Diagram

```
USER/Analyst
      â†“
  Jupyter Notebook
      â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  ETL PIPELINE   â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚  Extract        â”‚
  â”‚  â†“              â”‚
  â”‚  Transform      â”‚
  â”‚  â†“              â”‚
  â”‚  Load           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
    PostgreSQL DB
    (docker-compose)
           â†“
  SQL Queries / Reports
```

---

## Summary

**Architecture Highlights:**
âœ“ Modular design (Extract/Transform/Load)
âœ“ Star Schema for analytics
âœ“ Docker for easy deployment
âœ“ Pandas for data manipulation
âœ“ PostgreSQL for storage
âœ“ Jupyter for exploration
âœ“ Configuration management
âœ“ Error handling throughout

**Technology Stack:**

- Python 3.9+ (ETL logic)
- Pandas 2.1 (Data manipulation)
- PostgreSQL 15 (Database)
- Docker (Containerization)
- Jupyter (Interactive notebooks)
- SQLAlchemy (ORM)

---

**Last Updated:** November 16, 2025  
**For questions:** See README.md or FAQ.md
